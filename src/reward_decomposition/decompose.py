import matplotlib.pyplot as plt
import numpy as np
import torch as th


def train_decomposer(decomposer, batch, reward_optimizer):
    print("Training the Reward Decomposer")

    # organize the data
    reward_inputs, global_rewards = decomposer.build_data(batch)
    local_rewards = decomposer.forward(reward_inputs).reshape(-1, decomposer.n_agents)

    # compute the loss
    loss = compute_loss(local_rewards, global_rewards)
    print(loss)

    reward_optimizer.zero_grad()
    loss.backward()
    reward_optimizer.step()

    visualize_decomposer_1d(decomposer, batch)
    # visualize_data(reward_inputs, global_rewards)


# decomposes the global rewards into local rewards
# Returns (status, local_rewards) where status refers to the reliability of the local rewards
def decompose(decomposer, batch):
    print("Decomposing the global reward")

    # decompose the reward
    reward_inputs, global_rewards = decomposer.build_data(batch)
    local_rewards = decomposer.forward(reward_inputs).reshape(-1, decomposer.n_agents)

    # return the rewards and the status
    if check_decomposition(local_rewards, global_rewards):
        return True, local_rewards
    else:
        return False, None


# Function recvs the true global reward and the outputs generated by the decomposer, and checks how similar they are
# This will determine if the QLearner should use this modified sample or not
def check_decomposition(local_rewards, global_rewards):
    diff = compute_diff(local_rewards, global_rewards)
    return False

    print(diff)
    print("Check that the diff of every local reward is good")
    exit()

    return diff < 0.1  # TODO: change to threshold


def compute_loss(local_rewards, global_rewards):
    diff = compute_diff(local_rewards, global_rewards)
    loss = (diff ** 2).sum() / len(global_rewards)
    return loss


def compute_diff(local_rewards, global_rewards):
    summed_local_rewards = th.sum(local_rewards, dim=-1)
    global_rewards = th.reshape(global_rewards, summed_local_rewards.shape)
    return th.subtract(summed_local_rewards, global_rewards)


def visualize_decomposer_2d(decomposer, batch):
    example_input = decomposer.build_input(batch, 0, 0, 0)
    thetas = np.arange(-np.pi, np.pi, 0.1)
    reward = list()
    for theta in thetas:
        reward2 = []
        for x in thetas:
            example_input[0] = x
            example_input[2] = theta
            reward2.append(decomposer.forward(example_input).detach().numpy())
        reward.append(reward2)

    draw_2d_updating(reward)


def visualize_decomposer_1d(decomposer, batch):
    example_input = decomposer.build_input(batch, 0, 0, 0)
    xs = np.arange(-np.pi / 4, np.pi / 4, 0.01)
    ys = list()

    for theta in xs:
        example_input[0] = theta
        ys.append(decomposer.forward(example_input).detach().numpy())

    draw_1d_updating(xs, ys)


def visualize_batch(batch):
    xs = []
    ys = []

    for ep_idx in range(batch.batch_size):
        for t_idx in range(batch.max_seq_length):
            for agent_idx in range(batch["reward"][ep_idx, t_idx].shape[0]):
                print(batch["state"].shape)

                xs.append(batch["obs"][ep_idx, t_idx, agent_idx, 2].detach().numpy())
                ys.append(batch["reward"][ep_idx, t_idx, agent_idx].detach().numpy())

            # we include the step that led to the terminated state, but now exit
            if batch["terminated"][ep_idx, t_idx][0]:
                break

    draw_1d_updating(xs, ys)


def visualize_data(reward_inputs, global_rewards):
    xs = []
    ys = []

    for idx in range(len(reward_inputs)):
        xs.append(reward_inputs[idx, 0, 2].detach().numpy())
        ys.append(global_rewards[idx].detach().numpy())

    draw_1d_updating(xs, ys)


def draw_1d_updating(xs, ys):
    plt.clf()
    ax = plt.subplot()
    ax.scatter(xs, ys)
    plt.draw()
    plt.pause(0.001)


def draw_2d_updating(arr):
    plt.clf()
    ax = plt.subplot()
    im = ax.imshow(arr)

    # create an axes on the right side of ax. The width of cax will be 5%
    # of ax and the padding between cax and ax will be fixed at 0.05 inch.
    from mpl_toolkits.axes_grid1 import make_axes_locatable

    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.05)

    plt.colorbar(im, cax=cax)

    plt.draw()
    plt.pause(0.001)
