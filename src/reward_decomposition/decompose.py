import matplotlib.pyplot as plt
import numpy as np
import torch as th

import envs.multi_cart.constants as constants


def train_decomposer(decomposer, batch, reward_optimizer):
    # organize the data
    reward_inputs, global_rewards, mask, _ = build_reward_data(batch)
    local_rewards = decomposer.forward(reward_inputs)

    # compute the loss
    loss = compute_loss(local_rewards, global_rewards, mask)

    reward_optimizer.zero_grad()
    loss.backward()
    reward_optimizer.step()

    # visualize_decomposer_1d(decomposer, batch)
    # visualize_data(reward_inputs, global_rewards)


# decomposes the global rewards into local rewards
# Returns (status, reward_mask, local_rewards) where
# status - refers to the total reliability of the local rewards
# reward_mask - if status is True, which rewards should be used
def decompose(decomposer, batch):
    # print("Decomposing the global reward")

    # decompose the reward
    reward_inputs, global_rewards, mask, _ = build_reward_data(batch, include_last=False)
    local_rewards = decomposer.forward(reward_inputs)

    # now we assume that the local rewards
    status, reward_mask = build_reward_mask(decomposer, local_rewards, global_rewards, mask)

    # return the rewards and the status. Just in case, return None as local rewards to make sure they arent used even
    # if status is False
    if not status:
        local_rewards = None
    return status, reward_mask, local_rewards


# Function recvs the true global reward and the outputs generated by the decomposer, and checks how similar they are
# This will determine if the QLearner should use each modified sample
def build_reward_mask(decomposer, local_rewards, global_rewards, mask):
    diff = compute_diff(local_rewards, global_rewards, mask)

    # Determine the reward mask and the status
    reward_mask = th.where(th.logical_and(mask, (th.abs(diff) < decomposer.args.reward_diff_threshold)), 1., 0.)
    reward_decomposition_acc = th.sum(reward_mask) / th.sum(mask)
    status = reward_decomposition_acc > decomposer.args.reward_acc

    # Visualize inverse histogram if necessary
    print(f"Decomposition Accuracy: {reward_decomposition_acc}")
    # visualize_diff(diff, mask, horiz_line=decomposer.args.reward_diff_threshold)

    return status, reward_mask


def build_reward_data(batch, include_last=True):
    # For now, define the input for the reward decomposition network as just the observations
    # note that some of these aren't relevant, so we additionally supply a mask for pairs that shouldn't be learnt
    if include_last:
        inputs = batch["obs"]
        outputs = local_to_global(batch["reward"])
        truth = batch["reward"]
        mask = batch["filled"].float()
    else:
        inputs = batch["obs"][:, :-1]
        outputs = local_to_global(batch["reward"][:, :-1])
        truth = batch["reward"][:, :-1]
        mask = batch["filled"][:, :-1].float()

    return inputs, outputs, mask, truth


# huber loss
def huber(diff, delta=0.1):
    loss = th.where(th.abs(diff) < delta, 0.5 * (diff ** 2),
                    delta * th.abs(diff) - 0.5 * (delta ** 2))
    return th.sum(loss) / len(diff)


# log cosh loss
def logcosh(diff):
    loss = th.log(th.cosh(diff))
    return th.sum(loss) / len(diff)


def mse(diff):
    return th.sum(diff ** 2) / len(diff)


def mae(diff):
    return th.sum(th.abs(diff)) / len(diff)


def compute_loss(local_rewards, global_rewards, mask):
    diff = compute_diff(local_rewards, global_rewards, mask).flatten()
    loss = logcosh(diff)
    return loss


def compute_diff(local_rewards, global_rewards, mask):
    summed_local_rewards = local_to_global(local_rewards.squeeze())
    global_rewards = th.reshape(global_rewards, summed_local_rewards.shape)
    return th.mul(th.subtract(summed_local_rewards, global_rewards), mask)


def visualize_decomposer_2d(decomposer, batch):
    example_input = decomposer.build_input(batch, 0, 0, 0)
    thetas = np.arange(-np.pi, np.pi, 0.1)
    reward = list()
    for theta in thetas:
        reward2 = []
        for x in thetas:
            example_input[0] = x
            example_input[2] = theta
            reward2.append(decomposer.forward(example_input).detach().numpy())
        reward.append(reward2)

    draw_2d_updating(reward)


def visualize_decomposer_1d(decomposer, batch):
    example_input = decomposer.build_input(batch, 0, 0, 0)
    xs = np.arange(-np.pi / 4, np.pi / 4, 0.01)
    ys = list()

    for theta in xs:
        example_input[2] = theta
        ys.append(decomposer.forward(example_input).detach().numpy())

    draw_1d_updating(xs, ys)


def almost_flatten(arr):
    return arr.reshape(-1, arr.shape[-1])


def local_to_global(arr):
    return th.sum(arr, dim=-1, keepdims=True)


def visualize_batch(batch):
    reward_inputs, global_rewards, mask, real_local_rewards = build_reward_data(batch, include_last=False)

    # leave only theta, and expand
    reward_inputs = reward_inputs[:, :, :, 2]
    global_rewards = global_rewards.expand_as(reward_inputs)
    mask = mask.expand_as(reward_inputs)

    # reshape for visualization
    reward_inputs = reward_inputs.flatten()
    global_rewards = global_rewards.flatten()
    mask = mask.flatten()
    real_local_rewards = real_local_rewards.flatten()

    xs = []
    ys = []

    for obs_idx in range(len(reward_inputs)):
        # We include the step that led to the terminated state, but now exit
        # TODO: make sure this still happens with new build data
        if mask[obs_idx]:
            xs.append(reward_inputs[obs_idx])
            ys.append(real_local_rewards[obs_idx])

    plt.axvline(x=-constants.THETA_THRESHOLD_RADIANS)
    plt.axvline(x=constants.THETA_THRESHOLD_RADIANS)

    draw_1d_updating(xs, ys)


def visualize_diff(diff, mask, horiz_line=0.2):
    diff = diff.flatten()
    mask = mask.flatten()

    ys = []

    for diff_idx in range(len(diff)):
        if mask[diff_idx]:
            ys.append(diff[diff_idx].detach().numpy())

    plt.clf()
    plt.ylim()
    plt.axhline(y=horiz_line)
    plt.plot(np.sort(np.abs(ys)))
    plt.show()


def draw_1d_updating(xs, ys):
    ax = plt.subplot()
    ax.scatter(xs, ys)
    plt.ylim((-0.2, 1.2))
    plt.draw()
    plt.pause(0.001)


def draw_2d_updating(arr):
    plt.clf()
    ax = plt.subplot()
    im = ax.imshow(arr)

    # create an axes on the right side of ax. The width of cax will be 5%
    # of ax and the padding between cax and ax will be fixed at 0.05 inch.
    from mpl_toolkits.axes_grid1 import make_axes_locatable

    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.05)

    plt.colorbar(im, cax=cax)

    plt.draw()
    plt.pause(0.001)
