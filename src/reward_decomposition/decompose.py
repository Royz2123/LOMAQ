import matplotlib.pyplot as plt
import numpy as np
import torch as th
import torch.nn as nn
import itertools

import envs.multi_cart.constants as constants
from reward_decomposition.decomposer import RewardDecomposer
from mpl_toolkits.axes_grid1 import make_axes_locatable

import time


def train_decomposer(decomposer, batch, reward_optimizer):
    # organize the data
    reward_inputs, global_rewards, mask, _ = build_reward_data(batch)
    local_rewards = decomposer.forward(reward_inputs)

    if decomposer.args.assume_binary_reward:
        # First, create to global probabilities
        global_probs = decomposer.local_probs_to_global_probs(local_rewards)
        global_probs = th.log(global_probs + 1e-8)
        global_probs = th.reshape(global_probs, shape=(-1, global_probs.shape[-1]))

        # Next ready the global targets
        global_rewards = global_rewards.long()
        global_rewards = global_rewards.flatten()

        loss = nn.NLLLoss()
        output = loss(global_probs, global_rewards)

        local_rewards = decomposer.class_probs_to_local_rewards(local_rewards)
    else:
        # compute the loss
        local_rewards = th.stack(local_rewards, dim=2)
        output = compute_loss(local_rewards, global_rewards, mask)

    # Now add the regularizing loss
    reg_weights = decomposer.regularizing_weights
    reg_weights = th.reshape(reg_weights, shape=(1, 1, -1))
    reg_weights = reg_weights.repeat(*local_rewards.shape[:-1], 1)
    reg_loss = th.sum(th.square(local_rewards) * reg_weights)
    output += reg_loss

    reward_optimizer.zero_grad()
    output.backward()
    reward_optimizer.step()

    # visualize_decomposer_1d(decomposer, batch)
    # visualize_data(reward_inputs, global_rewards)


# decomposes the global rewards into local rewards
# Returns (status, reward_mask, local_rewards) where
# status - refers to the total reliability of the local rewards
# reward_mask - if status is True, which rewards should be used
def decompose(decomposer, batch):
    # print("Decomposing the global reward")

    # decompose the reward
    reward_inputs, global_rewards, mask, _ = build_reward_data(batch, include_last=False)
    local_rewards = decomposer.forward(reward_inputs)

    if decomposer.args.assume_binary_reward:
        global_rewards = global_rewards.long()
        local_rewards = decomposer.class_probs_to_local_rewards(local_rewards)
    else:
        local_rewards = th.stack(local_rewards, dim=2)

    agent_rewards = decomposer.local_rewards_to_agent_rewards(local_rewards)

    # now we assume that the local rewards
    status, reward_mask = build_reward_mask(decomposer, agent_rewards, global_rewards, mask)

    # return the rewards and the status. Just in case, return None as local rewards to make sure they arent used even
    # if status is False
    if not status:
        agent_rewards = None
    return status, reward_mask, agent_rewards


# Function recvs the true global reward and the outputs generated by the decomposer, and checks how similar they are
# This will determine if the QLearner should use each modified sample
def build_reward_mask(decomposer, local_rewards, global_rewards, mask):
    # if decomposer.args.assume_binary_reward:

    diff = compute_diff(local_rewards, global_rewards, mask)

    # Determine the reward mask and the status
    reward_mask = th.where(th.logical_and(mask, (th.abs(diff) < decomposer.args.reward_diff_threshold)), 1., 0.)
    reward_decomposition_acc = th.sum(reward_mask) / th.sum(mask)
    status = reward_decomposition_acc > decomposer.args.reward_acc

    # Visualize inverse histogram if necessary
    print(f"Decomposition Accuracy: {reward_decomposition_acc}")
    # visualize_diff(diff, mask, horiz_line=decomposer.args.reward_diff_threshold)

    return status, reward_mask


def build_reward_data(batch, include_last=True):
    # For now, define the input for the reward decomposition network as just the observations
    # note that some of these aren't relevant, so we additionally supply a mask for pairs that shouldn't be learnt
    if include_last:
        inputs = batch["obs"][:, :, :, -1:]
        outputs = local_to_global(batch["reward"])
        truth = batch["reward"]
        mask = batch["filled"].float()
    else:
        inputs = batch["obs"][:, :-1, :, -1:]
        outputs = local_to_global(batch["reward"][:, :-1])
        truth = batch["reward"][:, :-1]
        mask = batch["filled"][:, :-1].float()

    return inputs, outputs, mask, truth


# huber loss
def huber(diff, delta=0.1):
    loss = th.where(th.abs(diff) < delta, 0.5 * (diff ** 2),
                    delta * th.abs(diff) - 0.5 * (delta ** 2))
    return th.sum(loss) / len(diff)


# log cosh loss
def logcosh(diff):
    loss = th.log(th.cosh(diff))
    return th.sum(loss) / len(diff)


def mse(diff):
    return th.sum(diff ** 2) / len(diff)


def mae(diff):
    return th.sum(th.abs(diff)) / len(diff)


def compute_loss(local_rewards, global_rewards, mask):
    diff = compute_diff(local_rewards, global_rewards, mask).flatten()
    loss = mse(diff)
    return loss


def compute_diff(local_rewards, global_rewards, mask):
    # reshape local rewards
    local_rewards = th.reshape(local_rewards, shape=(*local_rewards.shape[:2], -1))
    summed_local_rewards = local_to_global(local_rewards)
    global_rewards = th.reshape(global_rewards, summed_local_rewards.shape)
    return th.mul(th.subtract(summed_local_rewards, global_rewards), mask)


def visualize_decomposer_2d(decomposer, batch, env_name="multi_particle"):
    example_input = decomposer.build_input(batch, 0, 0, 0)

    if env_name == "multi_particle":
        example_input_method = create_example_inputs_multi_particle
    elif env_name == "multi_cart":
        example_input_method = create_example_inputs_multi_cart
    else:
        print("Can't visualize decomposer for this enviroment...")
        return

    xs1, example_inputs1 = example_input_method(example_input)
    xs2, example_inputs2 = example_input_method(example_input)

    # Compute 1d outputs
    ys_1d = [decomposer.single_network_forward(reward_input, network_idx=0) for reward_input in example_inputs1]

    if decomposer.args.assume_binary_reward:
        ys_1d = [decomposer.probs_to_class_idx(reward_prob) for reward_prob in ys_1d]
        ys_1d = [y + min(decomposer.reward_networks[0].output_vals) for y in ys_1d]

    # Organize the 1d rewards for visualization
    ys_1d = np.array([reward_prob.detach().numpy() for reward_prob in ys_1d])
    ys_1d = np.reshape(ys_1d, (1, -1))
    ys_1d = np.repeat(ys_1d, xs1.shape[0])

    ys1_2d = np.reshape(ys_1d, (xs1.shape[0], xs1.shape[0]))
    ys2_2d = ys1_2d.T

    total_ys = ys2_2d + ys1_2d

    if len(decomposer.reward_networks) == 1:
        ys = None
    else:
        # Create a 2d array of all the examples
        xs = itertools.product(*[xs1, xs2])
        example_inputs = itertools.product(*[example_inputs1, example_inputs2])
        example_inputs = [th.cat((pair[0], pair[1])) for pair in example_inputs]

        # Run the 2d array through the last network
        ys = [decomposer.single_network_forward(reward_input, network_idx=1) for reward_input in example_inputs]

        if decomposer.args.assume_binary_reward:
            ys = [decomposer.probs_to_class_idx(reward_prob) for reward_prob in ys]
            ys = [y + min(decomposer.reward_networks[1].output_vals) for y in ys]

        ys = np.array([reward_prob.detach().numpy() for reward_prob in ys])
        ys = np.reshape(ys, (xs1.shape[0], xs1.shape[0]))

        total_ys += ys

    draw_multiple_2d([[ys, ys2_2d], [ys1_2d, total_ys]])

fig, ax = plt.subplots(ncols=2, nrows=2)

# For plotting purposes
def draw_multiple_2d(arr_list):
    for row_idx, row in enumerate(ax):
        for col_idx, col in enumerate(row):
            if arr_list[row_idx][col_idx] is None:
                continue
            im = col.imshow(arr_list[row_idx][col_idx], cmap='bone')
            divider = make_axes_locatable(col)
            cax = divider.append_axes('right', size='5%', pad=0.05)
            fig.colorbar(im, cax=cax, orientation='vertical')
    plt.draw()
    plt.pause(0.001)


# Returns the theta coordinate
def create_example_inputs_multi_cart(example_input):
    xs = np.arange(-np.pi / 4, np.pi / 4, 0.01)
    example_inputs = []
    for x_val in xs:
        temp_input = th.tensor(example_input)
        temp_input[2] = x_val
        example_inputs.append(temp_input)
    return xs, example_inputs


# Returns the theta coordinate
def create_example_inputs_multi_particle(example_input):
    xs = np.arange(0, 3, 0.1)
    example_inputs = []
    for x_val in xs:
        temp_input = th.tensor(example_input)
        temp_input[-1] = x_val
        # temp_input[-2] = x_val / np.sqrt(2)
        # temp_input[-3] = x_val / np.sqrt(2)
        example_inputs.append(temp_input)
    return xs, example_inputs


def visualize_decomposer_1d(decomposer, batch, env_name="multi_particle"):
    example_input = decomposer.build_input(batch, 0, 0, 0)

    if env_name == "multi_particle":
        xs, example_inputs = create_example_inputs_multi_particle(example_input)
    elif env_name == "multi_cart":
        xs, example_inputs = create_example_inputs_multi_cart(example_input)
    else:
        print("Can't visualize decomposer for this enviroment...")
        return

    ys = [decomposer.single_network_forward(reward_input) for reward_input in example_inputs]

    # If this is a classification problem, then change probability to class
    if decomposer.args.assume_binary_reward:
        # class idx is equal to the reward for single
        ys = [decomposer.probs_to_class_idx(reward_prob) for reward_prob in ys]

    ys = [reward_prob.detach().numpy() for reward_prob in ys]

    draw_1d_updating(xs, ys)


def almost_flatten(arr):
    return arr.reshape(-1, arr.shape[-1])


def local_to_global(arr):
    return th.sum(arr, dim=-1, keepdims=True)


# Returns the theta coordinate
def create_multi_cart_viz_input(reward_inputs):
    return reward_inputs[:, :, :, 2]


# # Returns the distance from the first landmark, which is the distance from the point in coordinates (4-5)
# def create_multi_particle_viz_input(reward_inputs):
#     return th.sqrt(th.sum(th.square(reward_inputs[:, :, :, 4:6]), dim=-1))

# Returns the distance from the first landmark, which is the distance from the point in coordinates (4-5)
def create_multi_particle_viz_input(reward_inputs):
    return reward_inputs[:, :, :, -1]


def visualize_batch_1d(batch, env_name="multi_particle"):
    reward_inputs, global_rewards, mask, real_local_rewards = build_reward_data(batch, include_last=False)

    # Get the x coordinate for the visualization
    if env_name == "multi_particle":
        reward_inputs = create_multi_particle_viz_input(reward_inputs)

        plt.axvline(x=0.3)
    elif env_name == "multi_cart":
        reward_inputs = create_multi_cart_viz_input(reward_inputs)

        plt.axvline(x=-constants.THETA_THRESHOLD_RADIANS)
        plt.axvline(x=constants.THETA_THRESHOLD_RADIANS)
    else:
        print("Can't visualize batch for this enviroment...")
        return

    global_rewards = global_rewards.expand_as(reward_inputs)
    mask = mask.expand_as(reward_inputs)

    # reshape for visualization
    reward_inputs = reward_inputs.flatten()
    global_rewards = global_rewards.flatten()
    mask = mask.flatten()
    real_local_rewards = real_local_rewards.flatten()

    xs = []
    ys = []

    for obs_idx in range(len(reward_inputs)):
        # We include the step that led to the terminated state, but now exit
        # TODO: make sure this still happens with new build data
        if mask[obs_idx]:
            xs.append(reward_inputs[obs_idx])
            ys.append(real_local_rewards[obs_idx])

    draw_1d_updating(xs, ys)


def visualize_diff(diff, mask, horiz_line=0.2):
    diff = diff.flatten()
    mask = mask.flatten()

    ys = []

    for diff_idx in range(len(diff)):
        if mask[diff_idx]:
            ys.append(diff[diff_idx].detach().numpy())

    plt.clf()
    plt.ylim()
    plt.axhline(y=horiz_line)
    plt.plot(np.sort(np.abs(ys)))
    plt.show()


def draw_1d_updating(xs, ys):
    plt.scatter(xs, ys)
    plt.ylim((-0.2, 1.2))
    plt.draw()
    plt.pause(0.001)
